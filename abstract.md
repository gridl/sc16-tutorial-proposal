# Abstract

In the pursuit of scientific research that is
more correct,
more widely applicable,
or simply needs to be done *faster*,
many researchers will run into  

* data that is too big to fit on their personal machines or lab workstations, and/or
* code that takes too long to run

The next step for these researchers is generally to get
access to a local HPC resource.
However, *before* they can use multi-threaded software,
MPI, GPUs, or run high-throughput analyses on large datasets,
they need to learn a different set of skills
to do so *effectively*,
such as task automation, cluster computing,
and managing parallel workflows.
This training is generally missing from coursework,
but offered by various groups,
including HPC center staff, system administrators, and
research computing "facilitators".

HPC Carpentry brings together these groups to design and deliver
a full day workshop
that teaches novice users these skills so that they
can quickly become productive in an HPC environment.
It is modeled on the lesson design and workshop practices
of the
[Software Carpentry](http://software-carpentry.org)
and
[Data Carpentry](http://www.datacarpentry.org)
projects,
which have found much success in teaching novice users
"basic lab skills" and best practices in
software development and data science.
